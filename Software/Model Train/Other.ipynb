{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7543e2-a7b7-4eb5-b4df-ee61e2187830",
   "metadata": {},
   "source": [
    "### 图神经网络环境配置，参见 https://www.bilibili.com/video/BV1184y1x71H?p=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a4dfa1-1d3a-44b4-8a3b-2f0edad319f9",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-20T18:31:05.141576Z",
     "start_time": "2024-04-20T18:31:02.555536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score\n",
    "from torch_geometric.nn import TopKPooling,SAGEConv, GCNConv, SAGPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as  gmp\n",
    "from torch.nn import Linear,BatchNorm1d, ReLU\n",
    "import torch.nn.functional as F\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3166d-a535-4f9e-a027-dfd958c13cd1",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1747f139-207a-4df3-807e-ba312940abd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T18:31:05.148187Z",
     "start_time": "2024-04-20T18:31:05.142325Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_num = 30  # 训练轮数\n",
    "lr = 0.001      # 学习率\n",
    "bs = 64         # 批次大小\n",
    "isCause = True # 是否加入因果关系\n",
    "\n",
    "dataset_root = '../Data' # 数据存放目录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be218d1-c38b-471f-8b6c-6c8ece728c40",
   "metadata": {},
   "source": [
    "## 因果关系的加入\n",
    "注意：节点必须从0开始标号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5993ffb9-c0ee-4149-982c-b3c63241bab9",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-20T18:31:05.148322Z",
     "start_time": "2024-04-20T18:31:05.146440Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_edge_index_and_edge_attr(ex_type):\n",
    "    # source_nodes->target_nodes对应着一条有向边，而edge_attr为此条有向边的权值,无向图代表两个方向都有边\n",
    "    source_nodes = [0, 1, 0, 4, 0, 7, 1, 2, 2, 3, 4, 5, 5, 6, 7, 8, 8, 9, 8, 14, 8, 19, 9, 10, 10, 11, 11, 12, 12, 13, 14, 15, 15, 16, 16, 17, 17, 18, 19, 20, 20, 21]\n",
    "    target_nodes = [1, 0, 4, 0, 7, 0, 2, 1, 3, 2, 5, 4, 6, 5, 8, 7, 9, 8, 14, 8, 19, 8, 10, 9, 11, 10, 12, 11, 13, 12, 15, 14, 16, 15, 17, 16, 18, 17, 20, 19, 21, 20]\n",
    "    edge_attr = None\n",
    "    if isCause:\n",
    "        # # one leg stand\n",
    "        # if ex_type == 1:\n",
    "        #     source_nodes = [0, 1, 2, 4, 6, 7, 8, 8 , 8 , 9, 10, 11, 12, 15, 15, 16, 17, 19, 19, 21]\n",
    "        #     target_nodes = [4, 0, 3, 5, 5, 0, 7, 9, 14, 10, 11, 12, 13, 14, 16, 17, 18, 8 , 20, 20]\n",
    "        #     edge_attr = None\n",
    "        if ex_type == 0:\n",
    "            source_nodes = [4, 1, 3, 5, 5, 7, 8, 8 , 8 , 9, 11, 11, 12, 14, 16, 16, 17, 19, 20, 20]\n",
    "            target_nodes = [0, 0, 2, 4, 6, 0, 7, 9, 14, 10, 10, 12, 13, 15, 15, 17, 18, 8 , 19, 21]\n",
    "            edge_attr = None\n",
    "    edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "    return edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff3b60-f1fe-4980-aa94-8f610d975f96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 数据处理部分\n",
    "注意：如果图结构发生改变，请删除数据根目录/processed/下的缓存文件，以重新进行数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1adfbbd6-edfe-47b5-8137-acfaf46a6900",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-20T18:31:14.289070Z",
     "start_time": "2024-04-20T18:31:05.155271Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isCause: True\n",
      "len of train_dataset: 143576\n",
      "len of test_dataset : 35895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 这个类继承自InMemoryDataset，专门处理中小型数据集\n",
    "class BinaryDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    # 如果根目录下的processed文件夹下已经尤处理好的图数据，则直接读取\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if isCause:\n",
    "            return ['cause.dataset']\n",
    "        else:\n",
    "            return ['base.dataset',]\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Load Data Seperate All data in Protective and Non-Prtective Data\n",
    "        \"\"\"\n",
    "        # Assuming the 'Data' folder is in the current working directory\n",
    "        data_folders = [dataset_root]\n",
    "\n",
    "        # Initialize lists to store the separated dataframes\n",
    "        protective_dfs = []\n",
    "        non_protective_dfs = []\n",
    "\n",
    "        # Loop through each data folder\n",
    "        for data_folder in data_folders:\n",
    "            # List all .mat files in the current data folder\n",
    "            mat_files = [f for f in os.listdir(data_folder) if f.endswith('.mat')]\n",
    "            # Load each mat file\n",
    "            for mat_file in mat_files:\n",
    "                # Construct the full path to the .mat file\n",
    "                mat_path = os.path.join(data_folder, mat_file)\n",
    "                # Load the .mat file\n",
    "                mat_data = loadmat(mat_path)\n",
    "                # Convert the data into a pandas dataframe\n",
    "                df = pd.DataFrame(mat_data['data'])\n",
    "                # Select only the first 70 columns and the last column (73rd) which contains the behavior label\n",
    "                df = df.iloc[:, list(range(66)) + [70] + [72]]\n",
    "                # Split the data based on the protective behavior label\n",
    "                # Assuming the last column in df is the protective behavior label\n",
    "                protective_behavior = df.iloc[:, -1]\n",
    "                protective_df = df[protective_behavior == 1]\n",
    "                non_protective_df = df[protective_behavior == 0]\n",
    "                # Append the resulting dataframes to their respective lists\n",
    "                protective_dfs.append(protective_df)\n",
    "                non_protective_dfs.append(non_protective_df)\n",
    "\n",
    "        # Concatenate all protective and non-protective dataframes\n",
    "        all_protective_data = pd.concat(protective_dfs, axis=0, ignore_index=True)\n",
    "        all_non_protective_data = pd.concat(non_protective_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "        # Now `all_protective_data` and `all_non_protective_data` hold the protective and non-protective data respectively\n",
    "        # You can process these dataframes as needed for your analysis or save them to new .mat files\n",
    "        # 保护和非保护数据个数 (77298, 68) (437247, 68)\n",
    "        # 数据合并\n",
    "        all_data = np.concatenate([all_protective_data,all_non_protective_data],axis=0)\n",
    "        # 提取运动类型和保护性行为列\n",
    "        ys = all_data[:,-2:]\n",
    "        # 将特征列构建为N*V*C的形式，其中N=图个数（一条数据就可以看作一个图），V代表节点个数， C代表每个节点的特征个数\n",
    "        x = all_data[:,np.newaxis,[0,22,44]]\n",
    "        for node in range(2, 23):\n",
    "            x_index = node - 1\n",
    "            y_index = x_index + 22\n",
    "            z_index = x_index + 44\n",
    "            temp = all_data[:,np.newaxis,[x_index,y_index,z_index]]\n",
    "            x = np.concatenate([x,temp], axis=1)\n",
    "        # 将边整理为GNN要求的格式，边索引必须从0开始\n",
    "       \n",
    "        # 构建数据列表\n",
    "        data_list = []\n",
    "        # 一条数据构建为一个图\n",
    "        for i in range(x.shape[0]):\n",
    "            y = torch.tensor([ys[i,-1]], dtype=torch.float)   \n",
    "            # 根据数据所属运动类型获得其拓扑结构\n",
    "            edge_index, edge_attr = get_edge_index_and_edge_attr(ys[i,-2])\n",
    "            if ys[i,-2] == 0:\n",
    "                data = Data(x=torch.tensor(x[i], dtype=torch.float), edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "                data_list.append(data)\n",
    "        # 将处理后的数据存储至指定根目录下\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "            \n",
    "        \n",
    "# 加载并分割数据集      \n",
    "def load_data_and_split(dataset_root):\n",
    "    # 数据打乱\n",
    "    dataset = BinaryDataset(root=dataset_root)\n",
    "    # 随机打乱\n",
    "    dataset = dataset.shuffle()\n",
    "    # 数据划分逻辑\n",
    "    num_graphs = len(dataset)\n",
    "    train_size = int(num_graphs * 0.8)\n",
    "    # train_size = int(num_graphs * 0.01)\n",
    "    \n",
    "    # 创建训练集和测试集的子集索引\n",
    "    train_indices = list(range(train_size))\n",
    "    test_indices = list(range(train_size, num_graphs))\n",
    "    \n",
    "    # 使用torch_geometric.data.Subset来划分数据\n",
    "    train_dataset = dataset.index_select(train_indices)\n",
    "    test_dataset = dataset.index_select(test_indices)\n",
    "    print(\"isCause:\",isCause)\n",
    "    print(\"len of train_dataset:\",len(train_dataset))\n",
    "    print(\"len of test_dataset :\",len(test_dataset))\n",
    "    # 创建DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader =  load_data_and_split(dataset_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ef746-3330-4fff-8f59-990873a0d412",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 模型定义\n",
    "模块参见文档 https://pytorch-geometric.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dadda6c-4d5c-4e06-bf0e-7cb1eab2304a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-20T18:31:14.294138Z",
     "start_time": "2024-04-20T18:31:14.291980Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(3,64)\n",
    "        self.pool1 = TopKPooling(64, ratio=0.8)\n",
    "        self.conv2 = GCNConv(64,64)\n",
    "        self.pool2 = TopKPooling(64, ratio=0.8)\n",
    "        self.conv3 = GCNConv(64,64)\n",
    "        self.pool3 = TopKPooling(64, ratio=0.8)\n",
    "        self.lin1 = Linear(64,64)\n",
    "        self.lin2 = Linear(64,32)\n",
    "        self.lin3 = Linear(32,1)\n",
    "        # self.bn1 = BatchNorm1d(64)\n",
    "        # self.bn2 = BatchNorm1d(32)\n",
    "        self.act1 = ReLU()\n",
    "        self.act2 = ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "\n",
    "        x = x.clone().detach().requires_grad_(True)\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        # x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, edge_attr, batch)\n",
    "        x1 = gap(x, batch)\n",
    "        \n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, edge_attr, batch)\n",
    "        x2 = gap(x, batch)\n",
    "        \n",
    "        # x = F.relu(self.conv3(x, edge_index))\n",
    "        # x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        # x3 = gap(x, batch)  \n",
    "        \n",
    "        x = x1 + x2\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act2(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # 输出值为0-1之间 （0为非保护 1为保护）\n",
    "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7da7e-d8ca-45fb-ae4a-2e407ad4f45b",
   "metadata": {},
   "source": [
    "## 模型训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68cb5da-0b83-4951-890e-0fcabdaeed9f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-20T18:42:06.783524Z",
     "start_time": "2024-04-20T18:31:14.295967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001  -------------------------- \n",
      "Train: accuracy: 0.9187, Macro Precision: 0.8674, F1: 0.2134, recall: 0.1228  \n",
      "Test : accuracy: 0.9173, Macro Precision: 0.8471, F1: 0.2077, recall: 0.1200\n",
      "Epoch: 002  -------------------------- \n",
      "Train: accuracy: 0.9217, Macro Precision: 0.8722, F1: 0.2728, recall: 0.1636  \n",
      "Test : accuracy: 0.9192, Macro Precision: 0.8496, F1: 0.2483, recall: 0.1477\n",
      "Epoch: 003  -------------------------- \n",
      "Train: accuracy: 0.9205, Macro Precision: 0.8595, F1: 0.2586, recall: 0.1544  \n",
      "Test : accuracy: 0.9192, Macro Precision: 0.8454, F1: 0.2521, recall: 0.1508\n",
      "Epoch: 004  -------------------------- \n",
      "Train: accuracy: 0.9250, Macro Precision: 0.8687, F1: 0.3405, recall: 0.2156  \n",
      "Test : accuracy: 0.9229, Macro Precision: 0.8548, F1: 0.3212, recall: 0.2020\n",
      "Epoch: 005  -------------------------- \n",
      "Train: accuracy: 0.9247, Macro Precision: 0.8875, F1: 0.3189, recall: 0.1963  \n",
      "Test : accuracy: 0.9235, Macro Precision: 0.8848, F1: 0.3068, recall: 0.1875\n",
      "Epoch: 006  -------------------------- \n",
      "Train: accuracy: 0.9232, Macro Precision: 0.8608, F1: 0.3134, recall: 0.1952  \n",
      "Test : accuracy: 0.9228, Macro Precision: 0.8633, F1: 0.3119, recall: 0.1936\n",
      "Epoch: 007  -------------------------- \n",
      "Train: accuracy: 0.9256, Macro Precision: 0.8602, F1: 0.3607, recall: 0.2336  \n",
      "Test : accuracy: 0.9237, Macro Precision: 0.8487, F1: 0.3439, recall: 0.2214\n",
      "Epoch: 008  -------------------------- \n",
      "Train: accuracy: 0.9284, Macro Precision: 0.8838, F1: 0.3864, recall: 0.2512  \n",
      "Test : accuracy: 0.9264, Macro Precision: 0.8722, F1: 0.3710, recall: 0.2402\n",
      "Epoch: 009  -------------------------- \n",
      "Train: accuracy: 0.9307, Macro Precision: 0.9222, F1: 0.3960, recall: 0.2528  \n",
      "Test : accuracy: 0.9297, Macro Precision: 0.9182, F1: 0.3882, recall: 0.2470\n",
      "Epoch: 010  -------------------------- \n",
      "Train: accuracy: 0.9309, Macro Precision: 0.8803, F1: 0.4311, recall: 0.2916  \n",
      "Test : accuracy: 0.9283, Macro Precision: 0.8653, F1: 0.4098, recall: 0.2757\n",
      "Epoch: 011  -------------------------- \n",
      "Train: accuracy: 0.9301, Macro Precision: 0.8427, F1: 0.4616, recall: 0.3339  \n",
      "Test : accuracy: 0.9272, Macro Precision: 0.8298, F1: 0.4374, recall: 0.3133\n",
      "Epoch: 012  -------------------------- \n",
      "Train: accuracy: 0.9323, Macro Precision: 0.8442, F1: 0.4968, recall: 0.3721  \n",
      "Test : accuracy: 0.9302, Macro Precision: 0.8355, F1: 0.4811, recall: 0.3583\n",
      "Epoch: 013  -------------------------- \n",
      "Train: accuracy: 0.9297, Macro Precision: 0.8788, F1: 0.4135, recall: 0.2759  \n",
      "Test : accuracy: 0.9279, Macro Precision: 0.8709, F1: 0.3979, recall: 0.2636\n",
      "Epoch: 014  -------------------------- \n",
      "Train: accuracy: 0.9326, Macro Precision: 0.8855, F1: 0.4522, recall: 0.3100  \n",
      "Test : accuracy: 0.9305, Macro Precision: 0.8731, F1: 0.4394, recall: 0.3013\n",
      "Epoch: 015  -------------------------- \n",
      "Train: accuracy: 0.9327, Macro Precision: 0.8252, F1: 0.5358, recall: 0.4324  \n",
      "Test : accuracy: 0.9306, Macro Precision: 0.8169, F1: 0.5244, recall: 0.4234\n",
      "Epoch: 016  -------------------------- \n",
      "Train: accuracy: 0.9390, Macro Precision: 0.8653, F1: 0.5666, recall: 0.4438  \n",
      "Test : accuracy: 0.9375, Macro Precision: 0.8578, F1: 0.5600, recall: 0.4403\n",
      "Epoch: 017  -------------------------- \n",
      "Train: accuracy: 0.9346, Macro Precision: 0.9065, F1: 0.4643, recall: 0.3158  \n",
      "Test : accuracy: 0.9329, Macro Precision: 0.8982, F1: 0.4531, recall: 0.3074\n",
      "Epoch: 018  -------------------------- \n",
      "Train: accuracy: 0.9323, Macro Precision: 0.8361, F1: 0.5089, recall: 0.3906  \n",
      "Test : accuracy: 0.9300, Macro Precision: 0.8251, F1: 0.4960, recall: 0.3814\n",
      "Epoch: 019  -------------------------- \n",
      "Train: accuracy: 0.9384, Macro Precision: 0.8479, F1: 0.5820, recall: 0.4773  \n",
      "Test : accuracy: 0.9364, Macro Precision: 0.8411, F1: 0.5692, recall: 0.4650\n",
      "Epoch: 020  -------------------------- \n",
      "Train: accuracy: 0.9411, Macro Precision: 0.8407, F1: 0.6294, recall: 0.5565  \n",
      "Test : accuracy: 0.9395, Macro Precision: 0.8350, F1: 0.6219, recall: 0.5510\n",
      "Epoch: 021  -------------------------- \n",
      "Train: accuracy: 0.9406, Macro Precision: 0.8663, F1: 0.5857, recall: 0.4678  \n",
      "Test : accuracy: 0.9394, Macro Precision: 0.8642, F1: 0.5773, recall: 0.4582\n",
      "Epoch: 022  -------------------------- \n",
      "Train: accuracy: 0.9361, Macro Precision: 0.8904, F1: 0.4999, recall: 0.3557  \n",
      "Test : accuracy: 0.9343, Macro Precision: 0.8833, F1: 0.4856, recall: 0.3435\n",
      "Epoch: 023  -------------------------- \n",
      "Train: accuracy: 0.9406, Macro Precision: 0.9093, F1: 0.5442, recall: 0.3949  \n",
      "Test : accuracy: 0.9394, Macro Precision: 0.9041, F1: 0.5372, recall: 0.3895\n",
      "Epoch: 024  -------------------------- \n",
      "Train: accuracy: 0.9400, Macro Precision: 0.8489, F1: 0.6016, recall: 0.5045  \n",
      "Test : accuracy: 0.9401, Macro Precision: 0.8510, F1: 0.6039, recall: 0.5057\n",
      "Epoch: 025  -------------------------- \n",
      "Train: accuracy: 0.9405, Macro Precision: 0.8777, F1: 0.5719, recall: 0.4426  \n",
      "Test : accuracy: 0.9390, Macro Precision: 0.8753, F1: 0.5601, recall: 0.4295\n",
      "Epoch: 026  -------------------------- \n",
      "Train: accuracy: 0.9351, Macro Precision: 0.8487, F1: 0.5330, recall: 0.4126  \n",
      "Test : accuracy: 0.9348, Macro Precision: 0.8512, F1: 0.5306, recall: 0.4080\n",
      "Epoch: 027  -------------------------- \n",
      "Train: accuracy: 0.9418, Macro Precision: 0.8759, F1: 0.5899, recall: 0.4664  \n",
      "Test : accuracy: 0.9407, Macro Precision: 0.8747, F1: 0.5820, recall: 0.4570\n",
      "Epoch: 028  -------------------------- \n",
      "Train: accuracy: 0.9421, Macro Precision: 0.8688, F1: 0.6020, recall: 0.4877  \n",
      "Test : accuracy: 0.9406, Macro Precision: 0.8656, F1: 0.5910, recall: 0.4752\n",
      "Epoch: 029  -------------------------- \n",
      "Train: accuracy: 0.9409, Macro Precision: 0.8782, F1: 0.5758, recall: 0.4470  \n",
      "Test : accuracy: 0.9399, Macro Precision: 0.8778, F1: 0.5687, recall: 0.4385\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        label = data.y.to(device)\n",
    "        loss = crit(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():  # 在评估模式下不计算梯度\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)  # 前向传播\n",
    "            pred = (out >= 0.5).int()\n",
    "\n",
    "            y_true.extend(data.y.clone().detach().tolist())  # 收集真实标签\n",
    "            y_pred.extend(pred.clone().detach().tolist())  # 收集预测标签\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    macro_precision = precision_score(y_true, y_pred,  pos_label=1, average='macro') \n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)  # Only report results for the class specified by pos_label. 考虑了非平衡\n",
    "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
    "    return accuracy, macro_precision, f1, recall\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "crit = torch.nn.BCELoss()\n",
    "for epoch in range(1, epoch_num):\n",
    "    train(train_loader)\n",
    "    train_accuracy, train_macro_precision, train_f1, train_recall = test(train_loader)\n",
    "    test_accuracy,  test_macro_precision,  test_f1 , test_recall  = test(test_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}  -------------------------- '\n",
    "          f'\\nTrain: accuracy: {train_accuracy:.4f}, Macro Precision: {train_macro_precision:.4f}, F1: {train_f1:.4f}, recall: {train_recall:.4f}  '\n",
    "          f'\\nTest : accuracy: {test_accuracy:.4f}, Macro Precision: {test_macro_precision:.4f}, F1: {test_f1:.4f}, recall: {test_recall:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
